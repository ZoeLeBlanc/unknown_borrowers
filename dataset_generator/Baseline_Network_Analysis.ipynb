{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network_analysis.generate_baseline_networks import *\n",
    "from network_analysis.load_datasets import get_updated_shxco_data\n",
    "import sys\n",
    "import warnings\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "from pyvis import network as net\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.readwrite import json_graph\n",
    "from networkx.algorithms import bipartite\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', None)\n",
    "# import community\n",
    "# import nx_altair as nxa\n",
    "# from node2vec import Node2Vec\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append(\"..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the stability of graphs in these groupings:\n",
    "- compare overall graph density\n",
    "- compare node metrics\n",
    "1. All Events **vs** All Events minus Exceptional Ones **vs** All Events minus those of Killen and Raphael France\n",
    "   1. Bipartite\n",
    "      1. All time\n",
    "      2. Seasonal\n",
    "      3. 1920s vs 1930s\n",
    "   2. Unipartite\n",
    "      1. All time\n",
    "      2. Seasonal\n",
    "      3. 1920s vs 1930s\n",
    "\n",
    "Goals:\n",
    "- identify communities\n",
    "- identify shape of network and how we want to divide it for analysis\n",
    "- find out which metrics are most useful for modeling subscribers and book reading habits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = borrow_events.copy()\n",
    "\n",
    "unexceptional_events = borrow_events[borrow_events.exceptional_types.isna()]\n",
    "\n",
    "no_rk_borrow_events = borrow_events[(borrow_events.member_id != 'killen') & (\n",
    "    borrow_events.member_id != 'raphael-france')]\n",
    "no_rk_members_df = members_df[(members_df.member_id != 'killen') & (\n",
    "    members_df.member_id != 'raphael-france')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipartite Comparisons\n",
    "\n",
    "#### Comparing Across Entire Time of Sco Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_attrs = {'uri': 'member_id'}\n",
    "book_attrs = {'uri': 'item_uri'}\n",
    "edge_attrs = {'weight': 'counts'}\n",
    "all_events_grouped = all_events.groupby(\n",
    "    ['member_id', 'item_uri']).size().reset_index(name='counts')\n",
    "unexceptional_events_grouped = unexceptional_events.groupby(\n",
    "    ['member_id', 'item_uri']).size().reset_index(name='counts')\n",
    "no_rk_events_grouped = no_rk_borrow_events.groupby(\n",
    "    ['member_id', 'item_uri']).size().reset_index(name='counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bipartite_graphs(grouped_events_df, member_attrs, book_attrs, edge_attrs, should_process, write_to_file, file_name, sk_metrics, link_metrics, members_df, books_df):\n",
    "    bipartite_graph = get_bipartite_graph(grouped_events_df, member_attrs, book_attrs, edge_attrs)\n",
    "    top_nodes = {n for n, d in bipartite_graph.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "    bottom_nodes = set(bipartite_graph) - top_nodes\n",
    "    print('graph density: ', bipartite.density(bipartite_graph, bottom_nodes))\n",
    "\n",
    "    if should_process:\n",
    "        processed_bipartite_graph = get_network_metrics(bipartite_graph, True)\n",
    "    else:\n",
    "        processed_bipartite_graph = bipartite_graph\n",
    "\n",
    "    processed_bipartite_nodelist, processed_bipartite_edgelist = generate_dataframes(processed_bipartite_graph, True)\n",
    "    print(f\"calculating local skmetrics: {' '.join(sk_metrics + link_metrics)}\")\n",
    "    local_nodelist = generate_local_metrics(processed_bipartite_graph,processed_bipartite_nodelist, sk_metrics, link_metrics, True)\n",
    "    print(f\"calculating global skmetrics: {' '.join(sk_metrics)}\")\n",
    "    updated_bipartite_nodelist = generate_sknetwork_metrics(processed_bipartite_edgelist, local_nodelist, metrics)\n",
    "    \n",
    "    print(F\"calculating global link metrics: : {' '.join(link_metrics)}\")\n",
    "    bipartite_nodelist = generate_link_metrics(processed_bipartite_graph, processed_bipartite_edgelist, updated_bipartite_nodelist, link_metrics, is_bipartite)\n",
    "    all_metrics = sk_metrics + link_metrics\n",
    "    for m in all_metrics:\n",
    "        bipartite_nodelist = bipartite_nodelist.rename(columns={m: f'global_{m}'})\n",
    "    if write_to_file:\n",
    "        bipartite_nodelist, bipartite_edgelist = write_dataframe(file_name, processed_bipartite_edgelist, bipartite_nodelist)\n",
    "    else:\n",
    "        bipartite_edgelist = processed_bipartite_edgelist\n",
    "    bipartite_members = bipartite_nodelist[bipartite_nodelist.group == 'members']\n",
    "\n",
    "    bipartite_books = bipartite_nodelist[bipartite_nodelist.group == 'books']\n",
    "\n",
    "    members_df['old_uri'] = members_df.uri\n",
    "    members_df.uri = members_df.member_id\n",
    "\n",
    "    joined_members = combine_dataframes(bipartite_members, members_df, bipartite_members.columns.tolist(), 'uri', 'inner')\n",
    "    joined_books = combine_dataframes(bipartite_books, books_df, bipartite_books.columns.tolist(), 'uri', 'inner')\n",
    "    \n",
    "    return (processed_bipartite_graph, bipartite_nodelist, bipartite_edgelist, joined_members, joined_books)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload Saved Graphs\n",
    "def reload_saved_graphs(file_path, members_df, books_df):\n",
    "    bipartite_graph = nx.read_gml(f'{file_path}_graph.gml')\n",
    "    bipartite_nodelist = pd.read_csv(f'{file_path}_nodelist.csv')\n",
    "    bipartite_edgelist = pd.read_csv(f'{file_path}_edgelist.csv')\n",
    "\n",
    "    bipartite_members = bipartite_nodelist[bipartite_nodelist.group == 'members']\n",
    "\n",
    "    bipartite_books = bipartite_nodelist[bipartite_nodelist.group == 'books']\n",
    "\n",
    "    members_df['old_uri'] = members_df.uri\n",
    "    members_df.uri = members_df.member_id\n",
    "\n",
    "    joined_members = combine_dataframes(\n",
    "        bipartite_members, members_df, bipartite_members.columns.tolist(), 'uri', 'inner')\n",
    "    joined_books = combine_dataframes(\n",
    "        bipartite_books, books_df, bipartite_books.columns.tolist(), 'uri', 'inner')\n",
    "\n",
    "    return (bipartite_graph, bipartite_nodelist, bipartite_edgelist, joined_members, joined_books)\n",
    "\n",
    "\n",
    "all_events_bipartite_graph, all_events_bipartite_nodelist, all_events_bipartite_edgelist, all_events_members, all_events_books = reload_saved_graphs('./data/all_events_bipartite', members_df, books_df)\n",
    "\n",
    "unexceptional_bipartite_graph, unexceptional_bipartite_nodelist, unexceptional_bipartite_edgelist, unexceptional_members, unexceptional_books = reload_saved_graphs('./data/unexceptional_bipartite', members_df, books_df)\n",
    "\n",
    "no_rk_bipartite_graph, no_rk_bipartite_nodelist, no_rk_bipartite_edgelist, no_rk_members, no_rk_books = reload_saved_graphs('./data/no_rk_bipartite', members_df, books_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['global_degree','local_degree', 'global_clustering', 'local_clustering',\n",
    "           'global_closeness', 'local_closeness', 'global_betweenness',\n",
    "           'local_betweenness','local_katz', 'local_hits', 'local_cohits',\n",
    "           'global_katz', 'global_hits', 'global_cohits']\n",
    "no_rk_books_corr = no_rk_books[columns].corr()\n",
    "all_events_books_corr = all_events_books[columns].corr()\n",
    "unexceptional_books_corr = unexceptional_books[columns].corr()\n",
    "no_rk_members_corr = no_rk_members[columns].corr()\n",
    "all_events_members_corr = all_events_members[columns].corr()\n",
    "unexceptional_members_corr = unexceptional_members[columns].corr()\n",
    "\n",
    "\n",
    "def generate_corr_chart(corr_df, title):\n",
    "    # data preparation\n",
    "    pivot_cols = list(corr_df.columns)\n",
    "    corr_df['cat'] = corr_df.index\n",
    "\n",
    "    base = alt.Chart(corr_df).transform_fold(pivot_cols).encode(\n",
    "        x=\"cat:N\",  y='key:N').properties(height=300, width=300, title=title)\n",
    "    boxes = base.mark_rect().encode(color=alt.Color(\n",
    "        \"value:Q\", scale=alt.Scale(scheme=\"redyellowblue\")))\n",
    "    labels = base.mark_text(size=5, color=\"grey\").encode(\n",
    "        text=alt.Text(\"value:Q\", format=\"0.1f\"))\n",
    "    chart = boxes + labels\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexceptional_corr_members_chart = generate_corr_chart(\n",
    "    unexceptional_members_corr, 'member correlations for unexceptional data')\n",
    "no_rk_corr_members_chart = generate_corr_chart(\n",
    "    no_rk_members_corr, 'member correlations for no rk data')\n",
    "all_events_corr_members_chart = generate_corr_chart(\n",
    "    all_events_members_corr, 'member correlations for all events data')\n",
    "\n",
    "alt.hconcat(*[all_events_corr_members_chart,\n",
    "            unexceptional_corr_members_chart, no_rk_corr_members_chart])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexceptional_corr_books_chart = generate_corr_chart(\n",
    "    unexceptional_books_corr, 'book correlations for unexceptional data')\n",
    "no_rk_corr_books_chart = generate_corr_chart(\n",
    "    no_rk_books_corr, 'book correlations for no rk data')\n",
    "all_events_corr_books_chart = generate_corr_chart(\n",
    "    all_events_books_corr, 'book correlations for all events data')\n",
    "\n",
    "alt.hconcat(*[all_events_corr_books_chart,\n",
    "            unexceptional_corr_books_chart, no_rk_corr_books_chart])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unipartite Comparisons\n",
    "\n",
    "#### Comparing Across Entire Time of Sco Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite_graph = get_bipartite_graph(\n",
    "    all_events_grouped, member_attrs, book_attrs, edge_attrs)\n",
    "member_nodes = [\n",
    "    n for n in bipartite_graph.nodes if bipartite_graph.nodes[n]['group'] == 'members']\n",
    "book_nodes = [\n",
    "    n for n in bipartite_graph.nodes if bipartite_graph.nodes[n]['group'] == 'books']\n",
    "projected_members_graph = bipartite.weighted_projected_graph(\n",
    "    bipartite_graph, member_nodes)\n",
    "projected_books_graph = bipartite.weighted_projected_graph(\n",
    "    bipartite_graph, book_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist, edgelist = generate_dataframes(bipartite_graph, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.pagerank(bipartite_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attrs = {}\n",
    "edge_attrs = {'item_uri': 'item_uri', 'weight': 'counts'}\n",
    "members_graph = nx.Graph()\n",
    "node_col = 'member_id'\n",
    "edge_col = 'item_uri'\n",
    "create_unipartite_network(borrow_events, members_graph,\n",
    "                          node_attrs, edge_attrs, node_col, edge_col)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
